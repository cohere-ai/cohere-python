# This file was auto-generated by Fern from our API Definition.

import json
import typing
from json.decoder import JSONDecodeError

import httpx_sse

from ..core.api_error import ApiError
from ..core.client_wrapper import AsyncClientWrapper, SyncClientWrapper
from ..core.request_options import RequestOptions
from ..core.unchecked_base_model import construct_type
from ..errors.bad_request_error import BadRequestError
from ..errors.client_closed_request_error import ClientClosedRequestError
from ..errors.forbidden_error import ForbiddenError
from ..errors.gateway_timeout_error import GatewayTimeoutError
from ..errors.internal_server_error import InternalServerError
from ..errors.not_found_error import NotFoundError
from ..errors.not_implemented_error import NotImplementedError
from ..errors.service_unavailable_error import ServiceUnavailableError
from ..errors.too_many_requests_error import TooManyRequestsError
from ..errors.unauthorized_error import UnauthorizedError
from ..errors.unprocessable_entity_error import UnprocessableEntityError
from ..types.client_closed_request_error_body import ClientClosedRequestErrorBody
from ..types.gateway_timeout_error_body import GatewayTimeoutErrorBody
from ..types.not_implemented_error_body import NotImplementedErrorBody
from ..types.too_many_requests_error_body import TooManyRequestsErrorBody
from ..types.unprocessable_entity_error_body import UnprocessableEntityErrorBody
from .types.chat_messages import ChatMessages
from .types.non_streamed_chat_response2 import NonStreamedChatResponse2
from .types.streamed_chat_response2 import StreamedChatResponse2
from .types.tool2 import Tool2
from .types.v2chat_request_citation_mode import V2ChatRequestCitationMode
from .types.v2chat_request_response_format import V2ChatRequestResponseFormat
from .types.v2chat_request_tool_choice import V2ChatRequestToolChoice
from .types.v2chat_request_truncation_mode import V2ChatRequestTruncationMode
from .types.v2chat_stream_request_citation_mode import V2ChatStreamRequestCitationMode
from .types.v2chat_stream_request_response_format import V2ChatStreamRequestResponseFormat
from .types.v2chat_stream_request_tool_choice import V2ChatStreamRequestToolChoice
from .types.v2chat_stream_request_truncation_mode import V2ChatStreamRequestTruncationMode

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class V2Client:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._client_wrapper = client_wrapper

    def chat_stream(
        self,
        *,
        model: str,
        messages: ChatMessages,
        tools: typing.Optional[typing.Sequence[Tool2]] = OMIT,
        tool_choice: typing.Optional[V2ChatStreamRequestToolChoice] = OMIT,
        citation_mode: typing.Optional[V2ChatStreamRequestCitationMode] = OMIT,
        truncation_mode: typing.Optional[V2ChatStreamRequestTruncationMode] = OMIT,
        response_format: typing.Optional[V2ChatStreamRequestResponseFormat] = OMIT,
        max_tokens: typing.Optional[int] = OMIT,
        stop_sequences: typing.Optional[typing.Sequence[str]] = OMIT,
        max_input_tokens: typing.Optional[int] = OMIT,
        temperature: typing.Optional[float] = OMIT,
        seed: typing.Optional[int] = OMIT,
        frequency_penalty: typing.Optional[float] = OMIT,
        presence_penalty: typing.Optional[float] = OMIT,
        k: typing.Optional[int] = OMIT,
        p: typing.Optional[int] = OMIT,
        return_prompt: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None
    ) -> typing.Iterator[StreamedChatResponse2]:
        """
        Parameters
        ----------
        model : str
            The model to use for the chat.

        messages : ChatMessages

        tools : typing.Optional[typing.Sequence[Tool2]]

        tool_choice : typing.Optional[V2ChatStreamRequestToolChoice]

        citation_mode : typing.Optional[V2ChatStreamRequestCitationMode]

        truncation_mode : typing.Optional[V2ChatStreamRequestTruncationMode]

        response_format : typing.Optional[V2ChatStreamRequestResponseFormat]

        max_tokens : typing.Optional[int]
            The maximum number of tokens to generate.

        stop_sequences : typing.Optional[typing.Sequence[str]]
            A list of strings that the model will stop generating at.

        max_input_tokens : typing.Optional[int]
            The maximum number of tokens to feed into the model.

        temperature : typing.Optional[float]
            The temperature of the model.

        seed : typing.Optional[int]

        frequency_penalty : typing.Optional[float]
            The frequency penalty of the model.

        presence_penalty : typing.Optional[float]
            The presence penalty of the model.

        k : typing.Optional[int]

        p : typing.Optional[int]

        return_prompt : typing.Optional[bool]
            Whether to return the prompt in the response.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Yields
        ------
        typing.Iterator[StreamedChatResponse2]


        Examples
        --------
        from cohere import (
            ChatMessage2_Assistant,
            Citation,
            Source_Tool,
            TextContent,
            Tool2,
            Tool2Function,
            ToolCall2,
            ToolCall2Function,
            V2ChatStreamRequestResponseFormat,
        )
        from cohere.client import Client

        client = Client(
            client_name="YOUR_CLIENT_NAME",
            token="YOUR_TOKEN",
        )
        response = client.v2.chat_stream(
            model="string",
            messages=[
                ChatMessage2_Assistant(
                    tool_calls=[
                        ToolCall2(
                            id="string",
                            function=ToolCall2Function(
                                name="string",
                                arguments="string",
                            ),
                        )
                    ],
                    tool_plan="string",
                    content=[
                        TextContent(
                            text="string",
                        )
                    ],
                    citations=[
                        Citation(
                            start="string",
                            end="string",
                            text="string",
                            sources=[
                                Source_Tool(
                                    id="string",
                                    tool_output={"string": {"key": "value"}},
                                )
                            ],
                        )
                    ],
                )
            ],
            tools=[
                Tool2(
                    function=Tool2Function(
                        name="string",
                        description="string",
                        parameters={"string": {"key": "value"}},
                    ),
                )
            ],
            tool_choice="AUTO",
            citation_mode="FAST",
            truncation_mode="OFF",
            response_format=V2ChatStreamRequestResponseFormat(
                schema={"string": {"key": "value"}},
            ),
            max_tokens=1,
            stop_sequences=["string"],
            max_input_tokens=1,
            temperature=1.1,
            seed=1,
            frequency_penalty=1.1,
            presence_penalty=1.1,
            k=1,
            p=1,
            return_prompt=True,
        )
        for chunk in response:
            yield chunk
        """
        with self._client_wrapper.httpx_client.stream(
            "v2/chat",
            method="POST",
            json={
                "model": model,
                "messages": messages,
                "tools": tools,
                "tool_choice": tool_choice,
                "citation_mode": citation_mode,
                "truncation_mode": truncation_mode,
                "response_format": response_format,
                "max_tokens": max_tokens,
                "stop_sequences": stop_sequences,
                "max_input_tokens": max_input_tokens,
                "temperature": temperature,
                "seed": seed,
                "frequency_penalty": frequency_penalty,
                "presence_penalty": presence_penalty,
                "k": k,
                "p": p,
                "return_prompt": return_prompt,
                "stream": True,
            },
            request_options=request_options,
            omit=OMIT,
        ) as _response:
            try:
                if 200 <= _response.status_code < 300:
                    _event_source = httpx_sse.EventSource(_response)
                    for _sse in _event_source.iter_sse():
                        try:
                            yield typing.cast(StreamedChatResponse2, construct_type(type_=StreamedChatResponse2, object_=json.loads(_sse.data)))  # type: ignore
                        except:
                            pass
                    return
                _response.read()
                if _response.status_code == 400:
                    raise BadRequestError(
                        typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                    )
                if _response.status_code == 401:
                    raise UnauthorizedError(
                        typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                    )
                if _response.status_code == 403:
                    raise ForbiddenError(
                        typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                    )
                if _response.status_code == 404:
                    raise NotFoundError(
                        typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                    )
                if _response.status_code == 422:
                    raise UnprocessableEntityError(
                        typing.cast(UnprocessableEntityErrorBody, construct_type(type_=UnprocessableEntityErrorBody, object_=_response.json()))  # type: ignore
                    )
                if _response.status_code == 429:
                    raise TooManyRequestsError(
                        typing.cast(TooManyRequestsErrorBody, construct_type(type_=TooManyRequestsErrorBody, object_=_response.json()))  # type: ignore
                    )
                if _response.status_code == 499:
                    raise ClientClosedRequestError(
                        typing.cast(ClientClosedRequestErrorBody, construct_type(type_=ClientClosedRequestErrorBody, object_=_response.json()))  # type: ignore
                    )
                if _response.status_code == 500:
                    raise InternalServerError(
                        typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                    )
                if _response.status_code == 501:
                    raise NotImplementedError(
                        typing.cast(NotImplementedErrorBody, construct_type(type_=NotImplementedErrorBody, object_=_response.json()))  # type: ignore
                    )
                if _response.status_code == 503:
                    raise ServiceUnavailableError(
                        typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                    )
                if _response.status_code == 504:
                    raise GatewayTimeoutError(
                        typing.cast(GatewayTimeoutErrorBody, construct_type(type_=GatewayTimeoutErrorBody, object_=_response.json()))  # type: ignore
                    )
                _response_json = _response.json()
            except JSONDecodeError:
                raise ApiError(status_code=_response.status_code, body=_response.text)
            raise ApiError(status_code=_response.status_code, body=_response_json)

    def chat(
        self,
        *,
        model: str,
        messages: ChatMessages,
        tools: typing.Optional[typing.Sequence[Tool2]] = OMIT,
        tool_choice: typing.Optional[V2ChatRequestToolChoice] = OMIT,
        citation_mode: typing.Optional[V2ChatRequestCitationMode] = OMIT,
        truncation_mode: typing.Optional[V2ChatRequestTruncationMode] = OMIT,
        response_format: typing.Optional[V2ChatRequestResponseFormat] = OMIT,
        max_tokens: typing.Optional[int] = OMIT,
        stop_sequences: typing.Optional[typing.Sequence[str]] = OMIT,
        max_input_tokens: typing.Optional[int] = OMIT,
        temperature: typing.Optional[float] = OMIT,
        seed: typing.Optional[int] = OMIT,
        frequency_penalty: typing.Optional[float] = OMIT,
        presence_penalty: typing.Optional[float] = OMIT,
        k: typing.Optional[int] = OMIT,
        p: typing.Optional[int] = OMIT,
        return_prompt: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None
    ) -> NonStreamedChatResponse2:
        """
        Parameters
        ----------
        model : str
            The model to use for the chat.

        messages : ChatMessages

        tools : typing.Optional[typing.Sequence[Tool2]]

        tool_choice : typing.Optional[V2ChatRequestToolChoice]

        citation_mode : typing.Optional[V2ChatRequestCitationMode]

        truncation_mode : typing.Optional[V2ChatRequestTruncationMode]

        response_format : typing.Optional[V2ChatRequestResponseFormat]

        max_tokens : typing.Optional[int]
            The maximum number of tokens to generate.

        stop_sequences : typing.Optional[typing.Sequence[str]]
            A list of strings that the model will stop generating at.

        max_input_tokens : typing.Optional[int]
            The maximum number of tokens to feed into the model.

        temperature : typing.Optional[float]
            The temperature of the model.

        seed : typing.Optional[int]

        frequency_penalty : typing.Optional[float]
            The frequency penalty of the model.

        presence_penalty : typing.Optional[float]
            The presence penalty of the model.

        k : typing.Optional[int]

        p : typing.Optional[int]

        return_prompt : typing.Optional[bool]
            Whether to return the prompt in the response.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        NonStreamedChatResponse2


        Examples
        --------
        from cohere.client import Client

        client = Client(
            client_name="YOUR_CLIENT_NAME",
            token="YOUR_TOKEN",
        )
        client.v2.chat(
            model="model",
            messages=[],
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "v2/chat",
            method="POST",
            json={
                "model": model,
                "messages": messages,
                "tools": tools,
                "tool_choice": tool_choice,
                "citation_mode": citation_mode,
                "truncation_mode": truncation_mode,
                "response_format": response_format,
                "max_tokens": max_tokens,
                "stop_sequences": stop_sequences,
                "max_input_tokens": max_input_tokens,
                "temperature": temperature,
                "seed": seed,
                "frequency_penalty": frequency_penalty,
                "presence_penalty": presence_penalty,
                "k": k,
                "p": p,
                "return_prompt": return_prompt,
                "stream": False,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(NonStreamedChatResponse2, construct_type(type_=NonStreamedChatResponse2, object_=_response.json()))  # type: ignore
            if _response.status_code == 400:
                raise BadRequestError(
                    typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                )
            if _response.status_code == 401:
                raise UnauthorizedError(
                    typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                )
            if _response.status_code == 403:
                raise ForbiddenError(
                    typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                )
            if _response.status_code == 404:
                raise NotFoundError(
                    typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(UnprocessableEntityErrorBody, construct_type(type_=UnprocessableEntityErrorBody, object_=_response.json()))  # type: ignore
                )
            if _response.status_code == 429:
                raise TooManyRequestsError(
                    typing.cast(TooManyRequestsErrorBody, construct_type(type_=TooManyRequestsErrorBody, object_=_response.json()))  # type: ignore
                )
            if _response.status_code == 499:
                raise ClientClosedRequestError(
                    typing.cast(ClientClosedRequestErrorBody, construct_type(type_=ClientClosedRequestErrorBody, object_=_response.json()))  # type: ignore
                )
            if _response.status_code == 500:
                raise InternalServerError(
                    typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                )
            if _response.status_code == 501:
                raise NotImplementedError(
                    typing.cast(NotImplementedErrorBody, construct_type(type_=NotImplementedErrorBody, object_=_response.json()))  # type: ignore
                )
            if _response.status_code == 503:
                raise ServiceUnavailableError(
                    typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                )
            if _response.status_code == 504:
                raise GatewayTimeoutError(
                    typing.cast(GatewayTimeoutErrorBody, construct_type(type_=GatewayTimeoutErrorBody, object_=_response.json()))  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)


class AsyncV2Client:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._client_wrapper = client_wrapper

    async def chat_stream(
        self,
        *,
        model: str,
        messages: ChatMessages,
        tools: typing.Optional[typing.Sequence[Tool2]] = OMIT,
        tool_choice: typing.Optional[V2ChatStreamRequestToolChoice] = OMIT,
        citation_mode: typing.Optional[V2ChatStreamRequestCitationMode] = OMIT,
        truncation_mode: typing.Optional[V2ChatStreamRequestTruncationMode] = OMIT,
        response_format: typing.Optional[V2ChatStreamRequestResponseFormat] = OMIT,
        max_tokens: typing.Optional[int] = OMIT,
        stop_sequences: typing.Optional[typing.Sequence[str]] = OMIT,
        max_input_tokens: typing.Optional[int] = OMIT,
        temperature: typing.Optional[float] = OMIT,
        seed: typing.Optional[int] = OMIT,
        frequency_penalty: typing.Optional[float] = OMIT,
        presence_penalty: typing.Optional[float] = OMIT,
        k: typing.Optional[int] = OMIT,
        p: typing.Optional[int] = OMIT,
        return_prompt: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None
    ) -> typing.AsyncIterator[StreamedChatResponse2]:
        """
        Parameters
        ----------
        model : str
            The model to use for the chat.

        messages : ChatMessages

        tools : typing.Optional[typing.Sequence[Tool2]]

        tool_choice : typing.Optional[V2ChatStreamRequestToolChoice]

        citation_mode : typing.Optional[V2ChatStreamRequestCitationMode]

        truncation_mode : typing.Optional[V2ChatStreamRequestTruncationMode]

        response_format : typing.Optional[V2ChatStreamRequestResponseFormat]

        max_tokens : typing.Optional[int]
            The maximum number of tokens to generate.

        stop_sequences : typing.Optional[typing.Sequence[str]]
            A list of strings that the model will stop generating at.

        max_input_tokens : typing.Optional[int]
            The maximum number of tokens to feed into the model.

        temperature : typing.Optional[float]
            The temperature of the model.

        seed : typing.Optional[int]

        frequency_penalty : typing.Optional[float]
            The frequency penalty of the model.

        presence_penalty : typing.Optional[float]
            The presence penalty of the model.

        k : typing.Optional[int]

        p : typing.Optional[int]

        return_prompt : typing.Optional[bool]
            Whether to return the prompt in the response.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Yields
        ------
        typing.AsyncIterator[StreamedChatResponse2]


        Examples
        --------
        import asyncio

        from cohere import (
            ChatMessage2_Assistant,
            Citation,
            Source_Tool,
            TextContent,
            Tool2,
            Tool2Function,
            ToolCall2,
            ToolCall2Function,
            V2ChatStreamRequestResponseFormat,
        )
        from cohere.client import AsyncClient

        client = AsyncClient(
            client_name="YOUR_CLIENT_NAME",
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            response = await client.v2.chat_stream(
                model="string",
                messages=[
                    ChatMessage2_Assistant(
                        tool_calls=[
                            ToolCall2(
                                id="string",
                                function=ToolCall2Function(
                                    name="string",
                                    arguments="string",
                                ),
                            )
                        ],
                        tool_plan="string",
                        content=[
                            TextContent(
                                text="string",
                            )
                        ],
                        citations=[
                            Citation(
                                start="string",
                                end="string",
                                text="string",
                                sources=[
                                    Source_Tool(
                                        id="string",
                                        tool_output={"string": {"key": "value"}},
                                    )
                                ],
                            )
                        ],
                    )
                ],
                tools=[
                    Tool2(
                        function=Tool2Function(
                            name="string",
                            description="string",
                            parameters={"string": {"key": "value"}},
                        ),
                    )
                ],
                tool_choice="AUTO",
                citation_mode="FAST",
                truncation_mode="OFF",
                response_format=V2ChatStreamRequestResponseFormat(
                    schema={"string": {"key": "value"}},
                ),
                max_tokens=1,
                stop_sequences=["string"],
                max_input_tokens=1,
                temperature=1.1,
                seed=1,
                frequency_penalty=1.1,
                presence_penalty=1.1,
                k=1,
                p=1,
                return_prompt=True,
            )
            async for chunk in response:
                yield chunk


        asyncio.run(main())
        """
        async with self._client_wrapper.httpx_client.stream(
            "v2/chat",
            method="POST",
            json={
                "model": model,
                "messages": messages,
                "tools": tools,
                "tool_choice": tool_choice,
                "citation_mode": citation_mode,
                "truncation_mode": truncation_mode,
                "response_format": response_format,
                "max_tokens": max_tokens,
                "stop_sequences": stop_sequences,
                "max_input_tokens": max_input_tokens,
                "temperature": temperature,
                "seed": seed,
                "frequency_penalty": frequency_penalty,
                "presence_penalty": presence_penalty,
                "k": k,
                "p": p,
                "return_prompt": return_prompt,
                "stream": True,
            },
            request_options=request_options,
            omit=OMIT,
        ) as _response:
            try:
                if 200 <= _response.status_code < 300:
                    _event_source = httpx_sse.EventSource(_response)
                    async for _sse in _event_source.aiter_sse():
                        try:
                            yield typing.cast(StreamedChatResponse2, construct_type(type_=StreamedChatResponse2, object_=json.loads(_sse.data)))  # type: ignore
                        except:
                            pass
                    return
                await _response.aread()
                if _response.status_code == 400:
                    raise BadRequestError(
                        typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                    )
                if _response.status_code == 401:
                    raise UnauthorizedError(
                        typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                    )
                if _response.status_code == 403:
                    raise ForbiddenError(
                        typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                    )
                if _response.status_code == 404:
                    raise NotFoundError(
                        typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                    )
                if _response.status_code == 422:
                    raise UnprocessableEntityError(
                        typing.cast(UnprocessableEntityErrorBody, construct_type(type_=UnprocessableEntityErrorBody, object_=_response.json()))  # type: ignore
                    )
                if _response.status_code == 429:
                    raise TooManyRequestsError(
                        typing.cast(TooManyRequestsErrorBody, construct_type(type_=TooManyRequestsErrorBody, object_=_response.json()))  # type: ignore
                    )
                if _response.status_code == 499:
                    raise ClientClosedRequestError(
                        typing.cast(ClientClosedRequestErrorBody, construct_type(type_=ClientClosedRequestErrorBody, object_=_response.json()))  # type: ignore
                    )
                if _response.status_code == 500:
                    raise InternalServerError(
                        typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                    )
                if _response.status_code == 501:
                    raise NotImplementedError(
                        typing.cast(NotImplementedErrorBody, construct_type(type_=NotImplementedErrorBody, object_=_response.json()))  # type: ignore
                    )
                if _response.status_code == 503:
                    raise ServiceUnavailableError(
                        typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                    )
                if _response.status_code == 504:
                    raise GatewayTimeoutError(
                        typing.cast(GatewayTimeoutErrorBody, construct_type(type_=GatewayTimeoutErrorBody, object_=_response.json()))  # type: ignore
                    )
                _response_json = _response.json()
            except JSONDecodeError:
                raise ApiError(status_code=_response.status_code, body=_response.text)
            raise ApiError(status_code=_response.status_code, body=_response_json)

    async def chat(
        self,
        *,
        model: str,
        messages: ChatMessages,
        tools: typing.Optional[typing.Sequence[Tool2]] = OMIT,
        tool_choice: typing.Optional[V2ChatRequestToolChoice] = OMIT,
        citation_mode: typing.Optional[V2ChatRequestCitationMode] = OMIT,
        truncation_mode: typing.Optional[V2ChatRequestTruncationMode] = OMIT,
        response_format: typing.Optional[V2ChatRequestResponseFormat] = OMIT,
        max_tokens: typing.Optional[int] = OMIT,
        stop_sequences: typing.Optional[typing.Sequence[str]] = OMIT,
        max_input_tokens: typing.Optional[int] = OMIT,
        temperature: typing.Optional[float] = OMIT,
        seed: typing.Optional[int] = OMIT,
        frequency_penalty: typing.Optional[float] = OMIT,
        presence_penalty: typing.Optional[float] = OMIT,
        k: typing.Optional[int] = OMIT,
        p: typing.Optional[int] = OMIT,
        return_prompt: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None
    ) -> NonStreamedChatResponse2:
        """
        Parameters
        ----------
        model : str
            The model to use for the chat.

        messages : ChatMessages

        tools : typing.Optional[typing.Sequence[Tool2]]

        tool_choice : typing.Optional[V2ChatRequestToolChoice]

        citation_mode : typing.Optional[V2ChatRequestCitationMode]

        truncation_mode : typing.Optional[V2ChatRequestTruncationMode]

        response_format : typing.Optional[V2ChatRequestResponseFormat]

        max_tokens : typing.Optional[int]
            The maximum number of tokens to generate.

        stop_sequences : typing.Optional[typing.Sequence[str]]
            A list of strings that the model will stop generating at.

        max_input_tokens : typing.Optional[int]
            The maximum number of tokens to feed into the model.

        temperature : typing.Optional[float]
            The temperature of the model.

        seed : typing.Optional[int]

        frequency_penalty : typing.Optional[float]
            The frequency penalty of the model.

        presence_penalty : typing.Optional[float]
            The presence penalty of the model.

        k : typing.Optional[int]

        p : typing.Optional[int]

        return_prompt : typing.Optional[bool]
            Whether to return the prompt in the response.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        NonStreamedChatResponse2


        Examples
        --------
        import asyncio

        from cohere.client import AsyncClient

        client = AsyncClient(
            client_name="YOUR_CLIENT_NAME",
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.v2.chat(
                model="model",
                messages=[],
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            "v2/chat",
            method="POST",
            json={
                "model": model,
                "messages": messages,
                "tools": tools,
                "tool_choice": tool_choice,
                "citation_mode": citation_mode,
                "truncation_mode": truncation_mode,
                "response_format": response_format,
                "max_tokens": max_tokens,
                "stop_sequences": stop_sequences,
                "max_input_tokens": max_input_tokens,
                "temperature": temperature,
                "seed": seed,
                "frequency_penalty": frequency_penalty,
                "presence_penalty": presence_penalty,
                "k": k,
                "p": p,
                "return_prompt": return_prompt,
                "stream": False,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(NonStreamedChatResponse2, construct_type(type_=NonStreamedChatResponse2, object_=_response.json()))  # type: ignore
            if _response.status_code == 400:
                raise BadRequestError(
                    typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                )
            if _response.status_code == 401:
                raise UnauthorizedError(
                    typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                )
            if _response.status_code == 403:
                raise ForbiddenError(
                    typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                )
            if _response.status_code == 404:
                raise NotFoundError(
                    typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(UnprocessableEntityErrorBody, construct_type(type_=UnprocessableEntityErrorBody, object_=_response.json()))  # type: ignore
                )
            if _response.status_code == 429:
                raise TooManyRequestsError(
                    typing.cast(TooManyRequestsErrorBody, construct_type(type_=TooManyRequestsErrorBody, object_=_response.json()))  # type: ignore
                )
            if _response.status_code == 499:
                raise ClientClosedRequestError(
                    typing.cast(ClientClosedRequestErrorBody, construct_type(type_=ClientClosedRequestErrorBody, object_=_response.json()))  # type: ignore
                )
            if _response.status_code == 500:
                raise InternalServerError(
                    typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                )
            if _response.status_code == 501:
                raise NotImplementedError(
                    typing.cast(NotImplementedErrorBody, construct_type(type_=NotImplementedErrorBody, object_=_response.json()))  # type: ignore
                )
            if _response.status_code == 503:
                raise ServiceUnavailableError(
                    typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                )
            if _response.status_code == 504:
                raise GatewayTimeoutError(
                    typing.cast(GatewayTimeoutErrorBody, construct_type(type_=GatewayTimeoutErrorBody, object_=_response.json()))  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

# This file was auto-generated by Fern from our API Definition.

import json
import typing
from json.decoder import JSONDecodeError

import httpx_sse

from ..core.api_error import ApiError
from ..core.client_wrapper import AsyncClientWrapper, SyncClientWrapper
from ..core.request_options import RequestOptions
from ..core.unchecked_base_model import construct_type
from ..errors.bad_request_error import BadRequestError
from ..errors.client_closed_request_error import ClientClosedRequestError
from ..errors.forbidden_error import ForbiddenError
from ..errors.gateway_timeout_error import GatewayTimeoutError
from ..errors.internal_server_error import InternalServerError
from ..errors.not_found_error import NotFoundError
from ..errors.not_implemented_error import NotImplementedError
from ..errors.service_unavailable_error import ServiceUnavailableError
from ..errors.too_many_requests_error import TooManyRequestsError
from ..errors.unauthorized_error import UnauthorizedError
from ..errors.unprocessable_entity_error import UnprocessableEntityError
from ..types.client_closed_request_error_body import ClientClosedRequestErrorBody
from ..types.gateway_timeout_error_body import GatewayTimeoutErrorBody
from ..types.not_implemented_error_body import NotImplementedErrorBody
from ..types.response_format2 import ResponseFormat2
from ..types.too_many_requests_error_body import TooManyRequestsErrorBody
from ..types.unprocessable_entity_error_body import UnprocessableEntityErrorBody
from .types.chat_messages import ChatMessages
from .types.non_streamed_chat_response2 import NonStreamedChatResponse2
from .types.streamed_chat_response2 import StreamedChatResponse2
from .types.tool2 import Tool2
from .types.v2chat_request_citation_mode import V2ChatRequestCitationMode
from .types.v2chat_stream_request_citation_mode import V2ChatStreamRequestCitationMode

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class V2Client:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._client_wrapper = client_wrapper

    def chat_stream(
        self,
        *,
        model: str,
        messages: ChatMessages,
        tools: typing.Optional[typing.Sequence[Tool2]] = OMIT,
        citation_mode: typing.Optional[V2ChatStreamRequestCitationMode] = OMIT,
        response_format: typing.Optional[ResponseFormat2] = OMIT,
        max_tokens: typing.Optional[int] = OMIT,
        stop_sequences: typing.Optional[typing.Sequence[str]] = OMIT,
        temperature: typing.Optional[float] = OMIT,
        seed: typing.Optional[int] = OMIT,
        frequency_penalty: typing.Optional[float] = OMIT,
        presence_penalty: typing.Optional[float] = OMIT,
        k: typing.Optional[float] = OMIT,
        p: typing.Optional[float] = OMIT,
        return_prompt: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None
    ) -> typing.Iterator[StreamedChatResponse2]:
        """
        Generates a message from the model in response to a provided conversation. To learn how to use the Chat API with Streaming and RAG follow our Text Generation guides.

        Parameters
        ----------
        model : str
            The name of a compatible [Cohere model](https://docs.cohere.com/docs/models) (such as command-r or command-r-plus) or the ID of a [fine-tuned](https://docs.cohere.com/docs/chat-fine-tuning) model.

        messages : ChatMessages

        tools : typing.Optional[typing.Sequence[Tool2]]
            A list of available tools (functions) that the model may suggest invoking before producing a text response.

            When `tools` is passed (without `tool_results`), the `text` content in the response will be empty and the `tool_calls` field in the response will be populated with a list of tool calls that need to be made. If no calls need to be made, the `tool_calls` array will be empty.


        citation_mode : typing.Optional[V2ChatStreamRequestCitationMode]
            Defaults to `"accurate"`.
            Dictates the approach taken to generating citations as part of the RAG flow by allowing the user to specify whether they want `"accurate"` results, `"fast"` results or no results.


        response_format : typing.Optional[ResponseFormat2]

        max_tokens : typing.Optional[int]
            The maximum number of tokens the model will generate as part of the response. Note: Setting a low value may result in incomplete generations.


        stop_sequences : typing.Optional[typing.Sequence[str]]
            A list of up to 5 strings that the model will use to stop generation. If the model generates a string that matches any of the strings in the list, it will stop generating tokens and return the generated text up to that point not including the stop sequence.


        temperature : typing.Optional[float]
            Defaults to `0.3`.

            A non-negative float that tunes the degree of randomness in generation. Lower temperatures mean less random generations, and higher temperatures mean more random generations.

            Randomness can be further maximized by increasing the  value of the `p` parameter.


        seed : typing.Optional[int]
            If specified, the backend will make a best effort to sample tokens
            deterministically, such that repeated requests with the same
            seed and parameters should return the same result. However,
            determinism cannot be totally guaranteed.


        frequency_penalty : typing.Optional[float]
            Defaults to `0.0`, min value of `0.0`, max value of `1.0`.
            Used to reduce repetitiveness of generated tokens. The higher the value, the stronger a penalty is applied to previously present tokens, proportional to how many times they have already appeared in the prompt or prior generation.


        presence_penalty : typing.Optional[float]
            Defaults to `0.0`, min value of `0.0`, max value of `1.0`.
            Used to reduce repetitiveness of generated tokens. Similar to `frequency_penalty`, except that this penalty is applied equally to all tokens that have already appeared, regardless of their exact frequencies.


        k : typing.Optional[float]
            Ensures only the top `k` most likely tokens are considered for generation at each step.
            Defaults to `0`, min value of `0`, max value of `500`.


        p : typing.Optional[float]
            Ensures that only the most likely tokens, with total probability mass of `p`, are considered for generation at each step. If both `k` and `p` are enabled, `p` acts after `k`.
            Defaults to `0.75`. min value of `0.01`, max value of `0.99`.


        return_prompt : typing.Optional[bool]
            Whether to return the prompt in the response.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Yields
        ------
        typing.Iterator[StreamedChatResponse2]


        Examples
        --------
        from cohere import ChatMessage2_User, ResponseFormat2_Text, Tool2, Tool2Function
        from cohere.client import Client

        client = Client(
            client_name="YOUR_CLIENT_NAME",
            token="YOUR_TOKEN",
        )
        response = client.v2.chat_stream(
            model="string",
            messages=[
                ChatMessage2_User(
                    content="string",
                    documents=[{"string": {"key": "value"}}],
                )
            ],
            tools=[
                Tool2(
                    function=Tool2Function(
                        name="string",
                        description="string",
                        parameters={"string": {"key": "value"}},
                    ),
                )
            ],
            citation_mode="FAST",
            response_format=ResponseFormat2_Text(),
            max_tokens=1,
            stop_sequences=["string"],
            temperature=1.1,
            seed=1,
            frequency_penalty=1.1,
            presence_penalty=1.1,
            k=1.1,
            p=1.1,
            return_prompt=True,
        )
        for chunk in response:
            yield chunk
        """
        with self._client_wrapper.httpx_client.stream(
            "v2/chat",
            method="POST",
            json={
                "model": model,
                "messages": messages,
                "tools": tools,
                "citation_mode": citation_mode,
                "response_format": response_format,
                "max_tokens": max_tokens,
                "stop_sequences": stop_sequences,
                "temperature": temperature,
                "seed": seed,
                "frequency_penalty": frequency_penalty,
                "presence_penalty": presence_penalty,
                "k": k,
                "p": p,
                "return_prompt": return_prompt,
                "stream": True,
            },
            request_options=request_options,
            omit=OMIT,
        ) as _response:
            try:
                if 200 <= _response.status_code < 300:
                    _event_source = httpx_sse.EventSource(_response)
                    for _sse in _event_source.iter_sse():
                        try:
                            yield typing.cast(StreamedChatResponse2, construct_type(type_=StreamedChatResponse2, object_=json.loads(_sse.data)))  # type: ignore
                        except:
                            pass
                    return
                _response.read()
                if _response.status_code == 400:
                    raise BadRequestError(
                        typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                    )
                if _response.status_code == 401:
                    raise UnauthorizedError(
                        typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                    )
                if _response.status_code == 403:
                    raise ForbiddenError(
                        typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                    )
                if _response.status_code == 404:
                    raise NotFoundError(
                        typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                    )
                if _response.status_code == 422:
                    raise UnprocessableEntityError(
                        typing.cast(UnprocessableEntityErrorBody, construct_type(type_=UnprocessableEntityErrorBody, object_=_response.json()))  # type: ignore
                    )
                if _response.status_code == 429:
                    raise TooManyRequestsError(
                        typing.cast(TooManyRequestsErrorBody, construct_type(type_=TooManyRequestsErrorBody, object_=_response.json()))  # type: ignore
                    )
                if _response.status_code == 499:
                    raise ClientClosedRequestError(
                        typing.cast(ClientClosedRequestErrorBody, construct_type(type_=ClientClosedRequestErrorBody, object_=_response.json()))  # type: ignore
                    )
                if _response.status_code == 500:
                    raise InternalServerError(
                        typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                    )
                if _response.status_code == 501:
                    raise NotImplementedError(
                        typing.cast(NotImplementedErrorBody, construct_type(type_=NotImplementedErrorBody, object_=_response.json()))  # type: ignore
                    )
                if _response.status_code == 503:
                    raise ServiceUnavailableError(
                        typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                    )
                if _response.status_code == 504:
                    raise GatewayTimeoutError(
                        typing.cast(GatewayTimeoutErrorBody, construct_type(type_=GatewayTimeoutErrorBody, object_=_response.json()))  # type: ignore
                    )
                _response_json = _response.json()
            except JSONDecodeError:
                raise ApiError(status_code=_response.status_code, body=_response.text)
            raise ApiError(status_code=_response.status_code, body=_response_json)

    def chat(
        self,
        *,
        model: str,
        messages: ChatMessages,
        tools: typing.Optional[typing.Sequence[Tool2]] = OMIT,
        citation_mode: typing.Optional[V2ChatRequestCitationMode] = OMIT,
        response_format: typing.Optional[ResponseFormat2] = OMIT,
        max_tokens: typing.Optional[int] = OMIT,
        stop_sequences: typing.Optional[typing.Sequence[str]] = OMIT,
        temperature: typing.Optional[float] = OMIT,
        seed: typing.Optional[int] = OMIT,
        frequency_penalty: typing.Optional[float] = OMIT,
        presence_penalty: typing.Optional[float] = OMIT,
        k: typing.Optional[float] = OMIT,
        p: typing.Optional[float] = OMIT,
        return_prompt: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None
    ) -> NonStreamedChatResponse2:
        """
        Generates a message from the model in response to a provided conversation. To learn how to use the Chat API with Streaming and RAG follow our Text Generation guides.

        Parameters
        ----------
        model : str
            The name of a compatible [Cohere model](https://docs.cohere.com/docs/models) (such as command-r or command-r-plus) or the ID of a [fine-tuned](https://docs.cohere.com/docs/chat-fine-tuning) model.

        messages : ChatMessages

        tools : typing.Optional[typing.Sequence[Tool2]]
            A list of available tools (functions) that the model may suggest invoking before producing a text response.

            When `tools` is passed (without `tool_results`), the `text` content in the response will be empty and the `tool_calls` field in the response will be populated with a list of tool calls that need to be made. If no calls need to be made, the `tool_calls` array will be empty.


        citation_mode : typing.Optional[V2ChatRequestCitationMode]
            Defaults to `"accurate"`.
            Dictates the approach taken to generating citations as part of the RAG flow by allowing the user to specify whether they want `"accurate"` results, `"fast"` results or no results.


        response_format : typing.Optional[ResponseFormat2]

        max_tokens : typing.Optional[int]
            The maximum number of tokens the model will generate as part of the response. Note: Setting a low value may result in incomplete generations.


        stop_sequences : typing.Optional[typing.Sequence[str]]
            A list of up to 5 strings that the model will use to stop generation. If the model generates a string that matches any of the strings in the list, it will stop generating tokens and return the generated text up to that point not including the stop sequence.


        temperature : typing.Optional[float]
            Defaults to `0.3`.

            A non-negative float that tunes the degree of randomness in generation. Lower temperatures mean less random generations, and higher temperatures mean more random generations.

            Randomness can be further maximized by increasing the  value of the `p` parameter.


        seed : typing.Optional[int]
            If specified, the backend will make a best effort to sample tokens
            deterministically, such that repeated requests with the same
            seed and parameters should return the same result. However,
            determinism cannot be totally guaranteed.


        frequency_penalty : typing.Optional[float]
            Defaults to `0.0`, min value of `0.0`, max value of `1.0`.
            Used to reduce repetitiveness of generated tokens. The higher the value, the stronger a penalty is applied to previously present tokens, proportional to how many times they have already appeared in the prompt or prior generation.


        presence_penalty : typing.Optional[float]
            Defaults to `0.0`, min value of `0.0`, max value of `1.0`.
            Used to reduce repetitiveness of generated tokens. Similar to `frequency_penalty`, except that this penalty is applied equally to all tokens that have already appeared, regardless of their exact frequencies.


        k : typing.Optional[float]
            Ensures only the top `k` most likely tokens are considered for generation at each step.
            Defaults to `0`, min value of `0`, max value of `500`.


        p : typing.Optional[float]
            Ensures that only the most likely tokens, with total probability mass of `p`, are considered for generation at each step. If both `k` and `p` are enabled, `p` acts after `k`.
            Defaults to `0.75`. min value of `0.01`, max value of `0.99`.


        return_prompt : typing.Optional[bool]
            Whether to return the prompt in the response.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        NonStreamedChatResponse2


        Examples
        --------
        from cohere.client import Client

        client = Client(
            client_name="YOUR_CLIENT_NAME",
            token="YOUR_TOKEN",
        )
        client.v2.chat(
            model="model",
            messages=[],
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "v2/chat",
            method="POST",
            json={
                "model": model,
                "messages": messages,
                "tools": tools,
                "citation_mode": citation_mode,
                "response_format": response_format,
                "max_tokens": max_tokens,
                "stop_sequences": stop_sequences,
                "temperature": temperature,
                "seed": seed,
                "frequency_penalty": frequency_penalty,
                "presence_penalty": presence_penalty,
                "k": k,
                "p": p,
                "return_prompt": return_prompt,
                "stream": False,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(NonStreamedChatResponse2, construct_type(type_=NonStreamedChatResponse2, object_=_response.json()))  # type: ignore
            if _response.status_code == 400:
                raise BadRequestError(
                    typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                )
            if _response.status_code == 401:
                raise UnauthorizedError(
                    typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                )
            if _response.status_code == 403:
                raise ForbiddenError(
                    typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                )
            if _response.status_code == 404:
                raise NotFoundError(
                    typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(UnprocessableEntityErrorBody, construct_type(type_=UnprocessableEntityErrorBody, object_=_response.json()))  # type: ignore
                )
            if _response.status_code == 429:
                raise TooManyRequestsError(
                    typing.cast(TooManyRequestsErrorBody, construct_type(type_=TooManyRequestsErrorBody, object_=_response.json()))  # type: ignore
                )
            if _response.status_code == 499:
                raise ClientClosedRequestError(
                    typing.cast(ClientClosedRequestErrorBody, construct_type(type_=ClientClosedRequestErrorBody, object_=_response.json()))  # type: ignore
                )
            if _response.status_code == 500:
                raise InternalServerError(
                    typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                )
            if _response.status_code == 501:
                raise NotImplementedError(
                    typing.cast(NotImplementedErrorBody, construct_type(type_=NotImplementedErrorBody, object_=_response.json()))  # type: ignore
                )
            if _response.status_code == 503:
                raise ServiceUnavailableError(
                    typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                )
            if _response.status_code == 504:
                raise GatewayTimeoutError(
                    typing.cast(GatewayTimeoutErrorBody, construct_type(type_=GatewayTimeoutErrorBody, object_=_response.json()))  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)


class AsyncV2Client:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._client_wrapper = client_wrapper

    async def chat_stream(
        self,
        *,
        model: str,
        messages: ChatMessages,
        tools: typing.Optional[typing.Sequence[Tool2]] = OMIT,
        citation_mode: typing.Optional[V2ChatStreamRequestCitationMode] = OMIT,
        response_format: typing.Optional[ResponseFormat2] = OMIT,
        max_tokens: typing.Optional[int] = OMIT,
        stop_sequences: typing.Optional[typing.Sequence[str]] = OMIT,
        temperature: typing.Optional[float] = OMIT,
        seed: typing.Optional[int] = OMIT,
        frequency_penalty: typing.Optional[float] = OMIT,
        presence_penalty: typing.Optional[float] = OMIT,
        k: typing.Optional[float] = OMIT,
        p: typing.Optional[float] = OMIT,
        return_prompt: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None
    ) -> typing.AsyncIterator[StreamedChatResponse2]:
        """
        Generates a message from the model in response to a provided conversation. To learn how to use the Chat API with Streaming and RAG follow our Text Generation guides.

        Parameters
        ----------
        model : str
            The name of a compatible [Cohere model](https://docs.cohere.com/docs/models) (such as command-r or command-r-plus) or the ID of a [fine-tuned](https://docs.cohere.com/docs/chat-fine-tuning) model.

        messages : ChatMessages

        tools : typing.Optional[typing.Sequence[Tool2]]
            A list of available tools (functions) that the model may suggest invoking before producing a text response.

            When `tools` is passed (without `tool_results`), the `text` content in the response will be empty and the `tool_calls` field in the response will be populated with a list of tool calls that need to be made. If no calls need to be made, the `tool_calls` array will be empty.


        citation_mode : typing.Optional[V2ChatStreamRequestCitationMode]
            Defaults to `"accurate"`.
            Dictates the approach taken to generating citations as part of the RAG flow by allowing the user to specify whether they want `"accurate"` results, `"fast"` results or no results.


        response_format : typing.Optional[ResponseFormat2]

        max_tokens : typing.Optional[int]
            The maximum number of tokens the model will generate as part of the response. Note: Setting a low value may result in incomplete generations.


        stop_sequences : typing.Optional[typing.Sequence[str]]
            A list of up to 5 strings that the model will use to stop generation. If the model generates a string that matches any of the strings in the list, it will stop generating tokens and return the generated text up to that point not including the stop sequence.


        temperature : typing.Optional[float]
            Defaults to `0.3`.

            A non-negative float that tunes the degree of randomness in generation. Lower temperatures mean less random generations, and higher temperatures mean more random generations.

            Randomness can be further maximized by increasing the  value of the `p` parameter.


        seed : typing.Optional[int]
            If specified, the backend will make a best effort to sample tokens
            deterministically, such that repeated requests with the same
            seed and parameters should return the same result. However,
            determinism cannot be totally guaranteed.


        frequency_penalty : typing.Optional[float]
            Defaults to `0.0`, min value of `0.0`, max value of `1.0`.
            Used to reduce repetitiveness of generated tokens. The higher the value, the stronger a penalty is applied to previously present tokens, proportional to how many times they have already appeared in the prompt or prior generation.


        presence_penalty : typing.Optional[float]
            Defaults to `0.0`, min value of `0.0`, max value of `1.0`.
            Used to reduce repetitiveness of generated tokens. Similar to `frequency_penalty`, except that this penalty is applied equally to all tokens that have already appeared, regardless of their exact frequencies.


        k : typing.Optional[float]
            Ensures only the top `k` most likely tokens are considered for generation at each step.
            Defaults to `0`, min value of `0`, max value of `500`.


        p : typing.Optional[float]
            Ensures that only the most likely tokens, with total probability mass of `p`, are considered for generation at each step. If both `k` and `p` are enabled, `p` acts after `k`.
            Defaults to `0.75`. min value of `0.01`, max value of `0.99`.


        return_prompt : typing.Optional[bool]
            Whether to return the prompt in the response.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Yields
        ------
        typing.AsyncIterator[StreamedChatResponse2]


        Examples
        --------
        import asyncio

        from cohere import ChatMessage2_User, ResponseFormat2_Text, Tool2, Tool2Function
        from cohere.client import AsyncClient

        client = AsyncClient(
            client_name="YOUR_CLIENT_NAME",
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            response = await client.v2.chat_stream(
                model="string",
                messages=[
                    ChatMessage2_User(
                        content="string",
                        documents=[{"string": {"key": "value"}}],
                    )
                ],
                tools=[
                    Tool2(
                        function=Tool2Function(
                            name="string",
                            description="string",
                            parameters={"string": {"key": "value"}},
                        ),
                    )
                ],
                citation_mode="FAST",
                response_format=ResponseFormat2_Text(),
                max_tokens=1,
                stop_sequences=["string"],
                temperature=1.1,
                seed=1,
                frequency_penalty=1.1,
                presence_penalty=1.1,
                k=1.1,
                p=1.1,
                return_prompt=True,
            )
            async for chunk in response:
                yield chunk


        asyncio.run(main())
        """
        async with self._client_wrapper.httpx_client.stream(
            "v2/chat",
            method="POST",
            json={
                "model": model,
                "messages": messages,
                "tools": tools,
                "citation_mode": citation_mode,
                "response_format": response_format,
                "max_tokens": max_tokens,
                "stop_sequences": stop_sequences,
                "temperature": temperature,
                "seed": seed,
                "frequency_penalty": frequency_penalty,
                "presence_penalty": presence_penalty,
                "k": k,
                "p": p,
                "return_prompt": return_prompt,
                "stream": True,
            },
            request_options=request_options,
            omit=OMIT,
        ) as _response:
            try:
                if 200 <= _response.status_code < 300:
                    _event_source = httpx_sse.EventSource(_response)
                    async for _sse in _event_source.aiter_sse():
                        try:
                            yield typing.cast(StreamedChatResponse2, construct_type(type_=StreamedChatResponse2, object_=json.loads(_sse.data)))  # type: ignore
                        except:
                            pass
                    return
                await _response.aread()
                if _response.status_code == 400:
                    raise BadRequestError(
                        typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                    )
                if _response.status_code == 401:
                    raise UnauthorizedError(
                        typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                    )
                if _response.status_code == 403:
                    raise ForbiddenError(
                        typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                    )
                if _response.status_code == 404:
                    raise NotFoundError(
                        typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                    )
                if _response.status_code == 422:
                    raise UnprocessableEntityError(
                        typing.cast(UnprocessableEntityErrorBody, construct_type(type_=UnprocessableEntityErrorBody, object_=_response.json()))  # type: ignore
                    )
                if _response.status_code == 429:
                    raise TooManyRequestsError(
                        typing.cast(TooManyRequestsErrorBody, construct_type(type_=TooManyRequestsErrorBody, object_=_response.json()))  # type: ignore
                    )
                if _response.status_code == 499:
                    raise ClientClosedRequestError(
                        typing.cast(ClientClosedRequestErrorBody, construct_type(type_=ClientClosedRequestErrorBody, object_=_response.json()))  # type: ignore
                    )
                if _response.status_code == 500:
                    raise InternalServerError(
                        typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                    )
                if _response.status_code == 501:
                    raise NotImplementedError(
                        typing.cast(NotImplementedErrorBody, construct_type(type_=NotImplementedErrorBody, object_=_response.json()))  # type: ignore
                    )
                if _response.status_code == 503:
                    raise ServiceUnavailableError(
                        typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                    )
                if _response.status_code == 504:
                    raise GatewayTimeoutError(
                        typing.cast(GatewayTimeoutErrorBody, construct_type(type_=GatewayTimeoutErrorBody, object_=_response.json()))  # type: ignore
                    )
                _response_json = _response.json()
            except JSONDecodeError:
                raise ApiError(status_code=_response.status_code, body=_response.text)
            raise ApiError(status_code=_response.status_code, body=_response_json)

    async def chat(
        self,
        *,
        model: str,
        messages: ChatMessages,
        tools: typing.Optional[typing.Sequence[Tool2]] = OMIT,
        citation_mode: typing.Optional[V2ChatRequestCitationMode] = OMIT,
        response_format: typing.Optional[ResponseFormat2] = OMIT,
        max_tokens: typing.Optional[int] = OMIT,
        stop_sequences: typing.Optional[typing.Sequence[str]] = OMIT,
        temperature: typing.Optional[float] = OMIT,
        seed: typing.Optional[int] = OMIT,
        frequency_penalty: typing.Optional[float] = OMIT,
        presence_penalty: typing.Optional[float] = OMIT,
        k: typing.Optional[float] = OMIT,
        p: typing.Optional[float] = OMIT,
        return_prompt: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None
    ) -> NonStreamedChatResponse2:
        """
        Generates a message from the model in response to a provided conversation. To learn how to use the Chat API with Streaming and RAG follow our Text Generation guides.

        Parameters
        ----------
        model : str
            The name of a compatible [Cohere model](https://docs.cohere.com/docs/models) (such as command-r or command-r-plus) or the ID of a [fine-tuned](https://docs.cohere.com/docs/chat-fine-tuning) model.

        messages : ChatMessages

        tools : typing.Optional[typing.Sequence[Tool2]]
            A list of available tools (functions) that the model may suggest invoking before producing a text response.

            When `tools` is passed (without `tool_results`), the `text` content in the response will be empty and the `tool_calls` field in the response will be populated with a list of tool calls that need to be made. If no calls need to be made, the `tool_calls` array will be empty.


        citation_mode : typing.Optional[V2ChatRequestCitationMode]
            Defaults to `"accurate"`.
            Dictates the approach taken to generating citations as part of the RAG flow by allowing the user to specify whether they want `"accurate"` results, `"fast"` results or no results.


        response_format : typing.Optional[ResponseFormat2]

        max_tokens : typing.Optional[int]
            The maximum number of tokens the model will generate as part of the response. Note: Setting a low value may result in incomplete generations.


        stop_sequences : typing.Optional[typing.Sequence[str]]
            A list of up to 5 strings that the model will use to stop generation. If the model generates a string that matches any of the strings in the list, it will stop generating tokens and return the generated text up to that point not including the stop sequence.


        temperature : typing.Optional[float]
            Defaults to `0.3`.

            A non-negative float that tunes the degree of randomness in generation. Lower temperatures mean less random generations, and higher temperatures mean more random generations.

            Randomness can be further maximized by increasing the  value of the `p` parameter.


        seed : typing.Optional[int]
            If specified, the backend will make a best effort to sample tokens
            deterministically, such that repeated requests with the same
            seed and parameters should return the same result. However,
            determinism cannot be totally guaranteed.


        frequency_penalty : typing.Optional[float]
            Defaults to `0.0`, min value of `0.0`, max value of `1.0`.
            Used to reduce repetitiveness of generated tokens. The higher the value, the stronger a penalty is applied to previously present tokens, proportional to how many times they have already appeared in the prompt or prior generation.


        presence_penalty : typing.Optional[float]
            Defaults to `0.0`, min value of `0.0`, max value of `1.0`.
            Used to reduce repetitiveness of generated tokens. Similar to `frequency_penalty`, except that this penalty is applied equally to all tokens that have already appeared, regardless of their exact frequencies.


        k : typing.Optional[float]
            Ensures only the top `k` most likely tokens are considered for generation at each step.
            Defaults to `0`, min value of `0`, max value of `500`.


        p : typing.Optional[float]
            Ensures that only the most likely tokens, with total probability mass of `p`, are considered for generation at each step. If both `k` and `p` are enabled, `p` acts after `k`.
            Defaults to `0.75`. min value of `0.01`, max value of `0.99`.


        return_prompt : typing.Optional[bool]
            Whether to return the prompt in the response.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        NonStreamedChatResponse2


        Examples
        --------
        import asyncio

        from cohere.client import AsyncClient

        client = AsyncClient(
            client_name="YOUR_CLIENT_NAME",
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.v2.chat(
                model="model",
                messages=[],
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            "v2/chat",
            method="POST",
            json={
                "model": model,
                "messages": messages,
                "tools": tools,
                "citation_mode": citation_mode,
                "response_format": response_format,
                "max_tokens": max_tokens,
                "stop_sequences": stop_sequences,
                "temperature": temperature,
                "seed": seed,
                "frequency_penalty": frequency_penalty,
                "presence_penalty": presence_penalty,
                "k": k,
                "p": p,
                "return_prompt": return_prompt,
                "stream": False,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(NonStreamedChatResponse2, construct_type(type_=NonStreamedChatResponse2, object_=_response.json()))  # type: ignore
            if _response.status_code == 400:
                raise BadRequestError(
                    typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                )
            if _response.status_code == 401:
                raise UnauthorizedError(
                    typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                )
            if _response.status_code == 403:
                raise ForbiddenError(
                    typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                )
            if _response.status_code == 404:
                raise NotFoundError(
                    typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(UnprocessableEntityErrorBody, construct_type(type_=UnprocessableEntityErrorBody, object_=_response.json()))  # type: ignore
                )
            if _response.status_code == 429:
                raise TooManyRequestsError(
                    typing.cast(TooManyRequestsErrorBody, construct_type(type_=TooManyRequestsErrorBody, object_=_response.json()))  # type: ignore
                )
            if _response.status_code == 499:
                raise ClientClosedRequestError(
                    typing.cast(ClientClosedRequestErrorBody, construct_type(type_=ClientClosedRequestErrorBody, object_=_response.json()))  # type: ignore
                )
            if _response.status_code == 500:
                raise InternalServerError(
                    typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                )
            if _response.status_code == 501:
                raise NotImplementedError(
                    typing.cast(NotImplementedErrorBody, construct_type(type_=NotImplementedErrorBody, object_=_response.json()))  # type: ignore
                )
            if _response.status_code == 503:
                raise ServiceUnavailableError(
                    typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_response.json()))  # type: ignore
                )
            if _response.status_code == 504:
                raise GatewayTimeoutError(
                    typing.cast(GatewayTimeoutErrorBody, construct_type(type_=GatewayTimeoutErrorBody, object_=_response.json()))  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

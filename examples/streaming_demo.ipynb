{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cef99a2c-02dc-49a3-aaea-392ab5b8d34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "sys.path.append(\"..\")  # make sure we can run this from the repo\n",
    "from IPython.display import Markdown, clear_output, display\n",
    "\n",
    "import cohere"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7348c95-7b3b-4ad4-b8cd-c600c8f2943a",
   "metadata": {},
   "source": [
    "## Client example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "927f90b4",
   "metadata": {},
   "source": [
    "### Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52ed8a61-f0de-489a-ad5e-9055fbd40781",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0][0.57s] StreamingText(id='774d85e3-92c6-47ce-b588-1bee2932adc7', index=0, text='\\n')\n",
      "[1][0.60s] StreamingText(id=None, index=0, text='\\n')\n",
      "[2][0.63s] StreamingText(id=None, index=0, text='I')\n",
      "[3][0.69s] StreamingText(id=None, index=0, text=' don')\n",
      "[4][0.69s] StreamingText(id=None, index=0, text=\"'t\")\n",
      "[5][0.87s] StreamingText(id=None, index=0, text=' know')\n",
      "[6][0.87s] StreamingText(id=None, index=0, text=' what')\n",
      "[7][0.87s] StreamingText(id=None, index=0, text=\"'s\")\n",
      "[8][0.87s] StreamingText(id=None, index=0, text=' going')\n",
      "[9][0.87s] StreamingText(id=None, index=0, text=' on')\n",
      "[10][0.87s] StreamingText(id=None, index=0, text=',')\n",
      "[11][0.89s] StreamingText(id=None, index=0, text=' but')\n",
      "[12][0.91s] StreamingText(id=None, index=0, text=' I')\n",
      "[13][0.95s] StreamingText(id=None, index=0, text=\"'m\")\n",
      "[14][0.97s] StreamingText(id=None, index=0, text=' really')\n",
      "[15][0.99s] StreamingText(id=None, index=0, text=' in')\n",
      "[16][1.06s] StreamingText(id=None, index=0, text=' a')\n",
      "[17][1.06s] StreamingText(id=None, index=0, text=' bad')\n",
      "[18][1.08s] StreamingText(id=None, index=0, text=' mood')\n",
      "[19][1.10s] StreamingText(id=None, index=0, text=' today')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['774d85e3-92c6-47ce-b588-1bee2932adc7'],\n",
       " [\"\\n\\nI don't know what's going on, but I'm really in a bad mood today\"])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co = cohere.Client()\n",
    "start_time = time.time()\n",
    "streaming_gens = co.generate(prompt=\"Hey! Don't worry, ğŸ happy~\", max_tokens=20, stream=True)\n",
    "for i, token in enumerate(streaming_gens):\n",
    "    print(f\"[{i}][{time.time()-start_time:.2f}s] {token}\")\n",
    "# the request id is available after the first token has streamed, and response so far is in texts\n",
    "streaming_gens.ids, streaming_gens.texts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a8140b9",
   "metadata": {},
   "source": [
    "### Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d40ceff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0][0.65s] StreamingText(index=0, text='That')\n",
      "[1][0.65s] StreamingText(index=0, text=\"'s\")\n",
      "[2][0.68s] StreamingText(index=0, text=' a')\n",
      "[3][0.71s] StreamingText(index=0, text=' tough')\n",
      "[4][0.76s] StreamingText(index=0, text=' question')\n",
      "[5][0.76s] StreamingText(index=0, text='.')\n",
      "[6][0.79s] StreamingText(index=0, text=' Ultimately')\n",
      "[7][0.82s] StreamingText(index=0, text=',')\n",
      "[8][0.85s] StreamingText(index=0, text=' it')\n",
      "[9][0.88s] StreamingText(index=0, text=\"'s\")\n",
      "[10][0.91s] StreamingText(index=0, text=' up')\n",
      "[11][1.01s] StreamingText(index=0, text=' to')\n",
      "[12][1.01s] StreamingText(index=0, text=' you')\n",
      "[13][1.01s] StreamingText(index=0, text=' to')\n",
      "[14][1.01s] StreamingText(index=0, text=' decide')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"That's a tough question. Ultimately, it's up to you to decide\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co = cohere.Client()\n",
    "start_time = time.time()\n",
    "streaming_chat = co.chat(query=\"Should I stay or should I go?\", max_tokens=15, stream=True)\n",
    "\n",
    "for i, token in enumerate(streaming_chat):\n",
    "    print(f\"[{i}][{time.time()-start_time:.2f}s] {token}\")\n",
    "\n",
    "streaming_chat.texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407077aa-c93f-4676-a1b4-b2b6e54f4cb6",
   "metadata": {},
   "source": [
    "## AsyncClient streaming example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d219446b-6788-48d8-8de4-af8f85e55cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Generation #1:** ì•ˆë…•í•˜ì„¸ìš” ì•ˆë…•í•˜ì„¸ìš” ì•ˆë…•í•˜ì„¸ìš” ì•ˆë…•í•˜ì„¸ìš” ì˜¤ëŠ˜ë„ ë©‹ì§„ ë‚ ì”¨ì…ë‹ˆë‹¤."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Generation #2:** ì•ˆë…•í•˜ì„¸ìš”. ë°˜ê°‘ìŠµë‹ˆë‹¤.\n",
       "Hello. Nice to meet you.\n",
       "\n",
       "Hi, nice to meet you too."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Generation #3:** ì•ˆë…•í•˜ì„¸ìš”! I've been a member of this community for a while, but haven't been as active as I would have liked to be. í•œêµ­ì–´ë¥¼ ê³µë¶€í•˜ê³  ìˆëŠ”"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Generation #4:** ì•ˆë…•í•˜ì„¸ìš”. ì—¬ê¸°ì„œëŠ” ìœ„ì˜ ì„¤ëª…ì„ ë”°ë¼ ìš°ë¦¬ì˜ ìƒˆë¡œìš´ ì§ì›ì´ ìŠ¤ìŠ¤ë¡œ ì •ê¸°ì "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Generation #5:** ì•ˆë…•í•˜ì„¸ìš”!\n",
       "I am learning Korean with Korean From Zero!\n",
       "So far, I have learned how to say Hello, Thank You, You're Welcome, I'm Fine, I'm Sorry, Good, Bad, and How are you"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"ì•ˆë…•í•˜\"\n",
    "num_generations = 5\n",
    "max_tokens = 50\n",
    "texts = [f\"**Generation #{i+1}:** {prompt}\" for i in range(num_generations)]\n",
    "displays = [display(display_id=True) for t in texts]\n",
    "\n",
    "async with cohere.AsyncClient() as aio_co:\n",
    "    start_time = time.time()\n",
    "    aio_streaming_gens = await aio_co.generate(\n",
    "        prompt=prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        model=\"command-xlarge-nightly\",\n",
    "        num_generations=num_generations,\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    async for token in aio_streaming_gens:\n",
    "        texts[token.index] += token.text\n",
    "        displays[token.index].update(Markdown(texts[token.index]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8d6cea95-c0ce-414a-9aa7-162bb8179e11",
   "metadata": {},
   "source": [
    "### Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fae3c5f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Yes, I would like to build a snowman."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"Do You Want to Build a Snowman?\"\n",
    "max_tokens = 15\n",
    "texts = \"\"\n",
    "displays = display(display_id=True)\n",
    "\n",
    "async with cohere.AsyncClient() as aio_co:\n",
    "    start_time = time.time()\n",
    "    streaming_chat = await aio_co.chat(\n",
    "        query=query,\n",
    "        max_tokens=max_tokens,\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    async for token in streaming_chat:\n",
    "        texts += token.text\n",
    "        displays.update(Markdown(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2a4272",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
